{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base path to the input dataset and then use it to derive\n",
    "# the path to the input images and annotation CSV files\n",
    "BASE_PATH = \"VOC2012\"\n",
    "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"JPEGimages\"])\n",
    "IMAGES_PATH_REDUCED = os.path.sep.join([BASE_PATH, \"Images_Reduced\"])\n",
    "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"Annotations\"])\n",
    "ANNOTS_PATH_REDUCED = os.path.sep.join([BASE_PATH, \"Annotations_Reduced\"])\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
    "# define the path to the output model, label encoder, plots output\n",
    "# directory, and testing image paths\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.pth\"])\n",
    "LE_PATH = os.path.sep.join([BASE_OUTPUT, \"le.pickle\"])\n",
    "PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to preprocess the data in a way that we have the two datasets we need. First, the whole dataset with images with several objects and parts and other with the images with only one object and one part in it. This is done because the Pytorch architecture only works with one object per image, but YOLO handles multiple objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_single_bounding_box(object_element):\n",
    "    # Check if an object has only one bounding box (ignoring parts)\n",
    "    return len(object_element.findall('part')) == 0\n",
    "\n",
    "def filter_annotations(input_folder_annotations, output_folder_annotations, input_folder_images, output_folder_images, limit_number = 10000):\n",
    "    os.makedirs(output_folder_annotations, exist_ok=True)\n",
    "    os.makedirs(output_folder_images, exist_ok=True)\n",
    "    count = 0\n",
    "    for filename in os.listdir(input_folder_annotations):\n",
    "        if count >= limit_number:\n",
    "            continue\n",
    "        if filename.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(input_folder_annotations, filename)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Check if the annotation has only one object with a single bounding box\n",
    "            objects = root.findall('.//object')\n",
    "            if len(objects) == 1 and has_single_bounding_box(objects[0]):\n",
    "                output_path = os.path.join(output_folder_annotations, filename)\n",
    "                # Copy the XML file to the output folder\n",
    "                os.makedirs(output_folder_annotations, exist_ok=True)\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(ET.tostring(root))\n",
    "                   # Copy the corresponding JPEG image to the \"JPEGImages\" folder\n",
    "                image_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "                image_path_src = os.path.join(input_folder_images, image_filename)\n",
    "                image_path_dest = os.path.join(output_folder_images, image_filename)\n",
    "                shutil.copy(image_path_src, image_path_dest)\n",
    "                count += 1 \n",
    "input_folder_a = ANNOTS_PATH\n",
    "output_folder_a = ANNOTS_PATH_REDUCED\n",
    "\n",
    "input_folder_i = IMAGES_PATH\n",
    "output_folder_i = IMAGES_PATH_REDUCED\n",
    "\n",
    "filter_annotations(input_folder_a, output_folder_a, input_folder_i, output_folder_i, limit_number = 4000)\n",
    "\n",
    "IMAGES_PATH_REDUCED_TEST = os.path.sep.join([BASE_PATH, \"Images_Reduced_Test\"])\n",
    "ANNOTS_PATH_REDUCED_TEST = os.path.sep.join([BASE_PATH, \"Annotations_Reduced_Test\"])\n",
    "\n",
    "input_folder_a = ANNOTS_PATH\n",
    "output_folder_a = ANNOTS_PATH_REDUCED_TEST\n",
    "\n",
    "input_folder_i = IMAGES_PATH\n",
    "output_folder_i = IMAGES_PATH_REDUCED_TEST\n",
    "\n",
    "filter_annotations(input_folder_a, output_folder_a, input_folder_i, output_folder_i, limit_number = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do it for the reduced dataset to compare with the other architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "data = []\n",
    "labels = []\n",
    "bboxes = []\n",
    "imagePaths = []\n",
    "\n",
    "# loop over all XML files in the annotations directory\n",
    "for xmlPath in paths.list_files(ANNOTS_PATH_REDUCED, validExts=(\".xml\")):\n",
    "    # load the contents of the current XML annotations file\n",
    "    tree = ET.parse(xmlPath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # extract information from XML\n",
    "    filename = root.find(\"filename\").text\n",
    "    label = root.find(\".//name\").text\n",
    "    xmin = float(root.find(\".//xmin\").text)\n",
    "    ymin = float(root.find(\".//ymin\").text)\n",
    "    xmax = float(root.find(\".//xmax\").text)\n",
    "    ymax = float(root.find(\".//ymax\").text)\n",
    "\n",
    "    # derive the path to the input image, load the image (in OpenCV format), and grab its dimensions\n",
    "    imagePath = os.path.sep.join([IMAGES_PATH_REDUCED, filename])\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # scale the bounding box coordinates relative to the spatial dimensions of the input image\n",
    "    startX = xmin / w\n",
    "    startY = ymin / h\n",
    "    endX = xmax / w\n",
    "    endY = ymax / h\n",
    "\n",
    "    # load the image and preprocess it\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # update our list of data, class labels, bounding boxes, and image paths\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    bboxes.append((startX, startY, endX, endY))\n",
    "    imagePaths.append(imagePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster processing of data, the lists are converted into numpy arrays. Since the labels are in string format, we use scikit-learnâ€™s LabelEncoder to transform them into their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data, class labels, bounding boxes, and image paths to\n",
    "# NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "imagePaths = np.array(imagePaths)\n",
    "# perform label encoding on the labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "# Initialize lists to store predictions and ground truth\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for image in imagePaths:\n",
    "    # Load image\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.resize(img, None, fx=0.4, fy=0.4)\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Showing informations on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(labels[class_ids[i]])\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0,255,0), 2)\n",
    "            cv2.putText(img, label, (x, y + 30), cv2.FONT_HERSHEY_PLAIN, 3, (0,255,0), 2)\n",
    "            \n",
    "    '''plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()'''\n",
    "\n",
    "    # Store predicted labels\n",
    "    predicted_labels = [labels[i] for i in class_ids]\n",
    "    #TODO: falta sacar las reales y probar todo\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "90/90 [==============================] - 78s 767ms/step - loss: 2.6541 - accuracy: 0.1788 - val_loss: 2.5196 - val_accuracy: 0.2375\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 63s 700ms/step - loss: 2.3378 - accuracy: 0.2531 - val_loss: 2.2720 - val_accuracy: 0.2656\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 64s 707ms/step - loss: 1.9919 - accuracy: 0.3684 - val_loss: 2.2181 - val_accuracy: 0.3156\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 64s 707ms/step - loss: 1.4277 - accuracy: 0.5396 - val_loss: 2.4942 - val_accuracy: 0.3187\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 64s 707ms/step - loss: 0.8084 - accuracy: 0.7424 - val_loss: 3.0815 - val_accuracy: 0.2594\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 64s 708ms/step - loss: 0.3854 - accuracy: 0.8764 - val_loss: 4.1009 - val_accuracy: 0.2844\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 64s 708ms/step - loss: 0.1849 - accuracy: 0.9476 - val_loss: 5.9183 - val_accuracy: 0.2688\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 64s 716ms/step - loss: 0.1399 - accuracy: 0.9556 - val_loss: 5.6775 - val_accuracy: 0.2625\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 63s 705ms/step - loss: 0.1111 - accuracy: 0.9722 - val_loss: 6.3503 - val_accuracy: 0.2937\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 63s 702ms/step - loss: 0.0492 - accuracy: 0.9868 - val_loss: 6.4101 - val_accuracy: 0.2656\n",
      "25/25 [==============================] - 4s 152ms/step\n",
      "Accuracy: 0.2725\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   aeroplane       0.56      0.67      0.61        82\n",
      "     bicycle       0.33      0.28      0.30        25\n",
      "        bird       0.20      0.24      0.22        85\n",
      "        boat       0.26      0.16      0.20        32\n",
      "      bottle       0.09      0.03      0.05        29\n",
      "         bus       0.17      0.06      0.08        18\n",
      "         car       0.22      0.27      0.24        52\n",
      "         cat       0.30      0.21      0.24       106\n",
      "       chair       0.00      0.00      0.00        15\n",
      "         cow       0.11      0.05      0.07        21\n",
      " diningtable       0.00      0.00      0.00         3\n",
      "         dog       0.23      0.49      0.31        92\n",
      "       horse       0.16      0.10      0.12        29\n",
      "   motorbike       0.15      0.17      0.16        24\n",
      "      person       0.16      0.10      0.12        62\n",
      " pottedplant       0.06      0.07      0.06        15\n",
      "       sheep       0.00      0.00      0.00        15\n",
      "        sofa       1.00      0.07      0.12        15\n",
      "       train       0.37      0.45      0.41        55\n",
      "   tvmonitor       0.44      0.28      0.34        25\n",
      "\n",
      "    accuracy                           0.27       800\n",
      "   macro avg       0.24      0.18      0.18       800\n",
      "weighted avg       0.27      0.27      0.25       800\n",
      "\n",
      "Confusion Matrix:\n",
      " [[55  0  7  5  0  0  2  0  0  0  0  2  1  1  2  0  0  0  6  1]\n",
      " [ 0  7  1  0  0  0  2  3  0  0  0  3  0  3  2  1  0  0  2  1]\n",
      " [14  2 20  1  3  0  4  7  1  1  0 21  0  0  8  1  0  0  0  2]\n",
      " [11  0  2  5  0  1  2  1  0  0  0  1  1  2  0  1  1  0  4  0]\n",
      " [ 0  0  4  1  1  0  3  4  2  0  0  6  1  1  3  1  0  0  2  0]\n",
      " [ 1  1  1  0  0  1  5  0  0  1  0  1  0  1  2  1  0  0  3  0]\n",
      " [ 7  1  4  2  0  0 14  1  3  1  0  2  1  2  1  2  1  0  9  1]\n",
      " [ 2  2 15  0  1  1  0 22  0  0  0 50  3  2  3  2  1  0  2  0]\n",
      " [ 0  1  1  1  1  1  2  1  0  0  0  4  0  1  1  0  0  0  1  0]\n",
      " [ 1  2  3  0  0  0  2  3  0  1  0  6  0  0  1  0  0  0  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  1  0  0  0  0]\n",
      " [ 1  0 11  1  1  0  5 14  1  0  0 45  4  2  2  2  1  0  1  1]\n",
      " [ 0  1  4  0  0  0  3  1  0  1  0 12  3  0  2  0  1  0  1  0]\n",
      " [ 0  0  4  1  0  0  3  2  1  0  0  4  1  4  0  1  1  0  2  0]\n",
      " [ 3  1  8  1  0  0  5 10  2  0  0 20  1  1  6  3  0  0  1  0]\n",
      " [ 1  1  4  0  1  0  3  0  0  0  0  0  0  3  0  1  0  0  1  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  3  0  7  1  0  0  1  0  0  1  0]\n",
      " [ 1  0  2  0  1  0  0  0  0  0  0  6  0  0  0  0  0  1  3  1]\n",
      " [ 2  2  2  0  1  1  7  1  0  1  0  2  2  4  4  0  0  0 25  1]\n",
      " [ 0  0  3  1  1  1  2  4  0  0  0  3  0  0  0  0  0  0  3  7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess the data and labels\n",
    "trainData /= 255.0\n",
    "testData /= 255.0\n",
    "trainLabels = to_categorical(trainLabels, num_classes=len(le.classes_))\n",
    "testLabels = to_categorical(testLabels, num_classes=len(le.classes_))\n",
    "\n",
    "# Create a simple CNN model (you may need to adjust the architecture based on your data)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainData, trainLabels, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = model.predict(testData)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(np.argmax(testLabels, axis=1), predicted_labels)\n",
    "classification_report_str = classification_report(np.argmax(testLabels, axis=1), predicted_labels, target_names=le.classes_)\n",
    "confusion_mat = confusion_matrix(np.argmax(testLabels, axis=1), predicted_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report_str)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
