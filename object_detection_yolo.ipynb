{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base path to the input dataset and then use it to derive\n",
    "# the path to the input images and annotation CSV files\n",
    "BASE_PATH = \"VOC2012\"\n",
    "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"JPEGimages\"])\n",
    "IMAGES_PATH_REDUCED = os.path.sep.join([BASE_PATH, \"Images_Reduced\"])\n",
    "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"Annotations\"])\n",
    "ANNOTS_PATH_REDUCED = os.path.sep.join([BASE_PATH, \"Annotations_Reduced\"])\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
    "# define the path to the output model, label encoder, plots output\n",
    "# directory, and testing image paths\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.pth\"])\n",
    "LE_PATH = os.path.sep.join([BASE_OUTPUT, \"le.pickle\"])\n",
    "PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to preprocess the data in a way that we have the two datasets we need. First, the whole dataset with images with several objects and parts and other with the images with only one object and one part in it. This is done because the Pytorch architecture only works with one object per image, but YOLO handles multiple objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_single_bounding_box(object_element):\n",
    "    # Check if an object has only one bounding box (ignoring parts)\n",
    "    return len(object_element.findall('part')) == 0\n",
    "\n",
    "def filter_annotations(input_folder_annotations, output_folder_annotations, input_folder_images, output_folder_images, limit_number = 10000):\n",
    "    os.makedirs(output_folder_annotations, exist_ok=True)\n",
    "    os.makedirs(output_folder_images, exist_ok=True)\n",
    "    count = 0\n",
    "    for filename in os.listdir(input_folder_annotations):\n",
    "        if count >= limit_number:\n",
    "            continue\n",
    "        if filename.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(input_folder_annotations, filename)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Check if the annotation has only one object with a single bounding box\n",
    "            objects = root.findall('.//object')\n",
    "            if len(objects) == 1 and has_single_bounding_box(objects[0]):\n",
    "                output_path = os.path.join(output_folder_annotations, filename)\n",
    "                # Copy the XML file to the output folder\n",
    "                os.makedirs(output_folder_annotations, exist_ok=True)\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(ET.tostring(root))\n",
    "                   # Copy the corresponding JPEG image to the \"JPEGImages\" folder\n",
    "                image_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "                image_path_src = os.path.join(input_folder_images, image_filename)\n",
    "                image_path_dest = os.path.join(output_folder_images, image_filename)\n",
    "                shutil.copy(image_path_src, image_path_dest)\n",
    "                count += 1 \n",
    "input_folder_a = ANNOTS_PATH\n",
    "output_folder_a = ANNOTS_PATH_REDUCED\n",
    "\n",
    "input_folder_i = IMAGES_PATH\n",
    "output_folder_i = IMAGES_PATH_REDUCED\n",
    "\n",
    "filter_annotations(input_folder_a, output_folder_a, input_folder_i, output_folder_i, limit_number = 4000)\n",
    "\n",
    "IMAGES_PATH_REDUCED_TEST = os.path.sep.join([BASE_PATH, \"Images_Reduced_Test\"])\n",
    "ANNOTS_PATH_REDUCED_TEST = os.path.sep.join([BASE_PATH, \"Annotations_Reduced_Test\"])\n",
    "\n",
    "input_folder_a = ANNOTS_PATH\n",
    "output_folder_a = ANNOTS_PATH_REDUCED_TEST\n",
    "\n",
    "input_folder_i = IMAGES_PATH\n",
    "output_folder_i = IMAGES_PATH_REDUCED_TEST\n",
    "\n",
    "filter_annotations(input_folder_a, output_folder_a, input_folder_i, output_folder_i, limit_number = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do it for the reduced dataset to compare with the other architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "data = []\n",
    "labels = []\n",
    "bboxes = []\n",
    "imagePaths = []\n",
    "\n",
    "# loop over all XML files in the annotations directory\n",
    "for xmlPath in paths.list_files(ANNOTS_PATH_REDUCED, validExts=(\".xml\")):\n",
    "    # load the contents of the current XML annotations file\n",
    "    tree = ET.parse(xmlPath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # extract information from XML\n",
    "    filename = root.find(\"filename\").text\n",
    "    label = root.find(\".//name\").text\n",
    "    xmin = float(root.find(\".//xmin\").text)\n",
    "    ymin = float(root.find(\".//ymin\").text)\n",
    "    xmax = float(root.find(\".//xmax\").text)\n",
    "    ymax = float(root.find(\".//ymax\").text)\n",
    "\n",
    "    # derive the path to the input image, load the image (in OpenCV format), and grab its dimensions\n",
    "    imagePath = os.path.sep.join([IMAGES_PATH_REDUCED, filename])\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # scale the bounding box coordinates relative to the spatial dimensions of the input image\n",
    "    startX = xmin / w\n",
    "    startY = ymin / h\n",
    "    endX = xmax / w\n",
    "    endY = ymax / h\n",
    "\n",
    "    # load the image and preprocess it\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # update our list of data, class labels, bounding boxes, and image paths\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    bboxes.append((startX, startY, endX, endY))\n",
    "    imagePaths.append(imagePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster processing of data, the lists are converted into numpy arrays. Since the labels are in string format, we use scikit-learnâ€™s LabelEncoder to transform them into their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data, class labels, bounding boxes, and image paths to\n",
    "# NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "imagePaths = np.array(imagePaths)\n",
    "# perform label encoding on the labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "labels = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant',\n",
    "          'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "          'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "          'toothbrush']\n",
    "\n",
    "# Function to get bounding boxes and detected classes\n",
    "def get_objects(image):\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Get output layers\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    layer_outputs = net.forward(output_layers_names)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x, center_y, w, h = (detection[0:4] * np.array([width, height, width, height])).astype(int)\n",
    "                x, y = int(center_x - w/2), int(center_y - h/2)\n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    return boxes, confidences, class_ids\n",
    "\n",
    "# Function to draw bounding boxes on the image\n",
    "def draw_boxes(image, boxes, confidences, class_ids):\n",
    "    colors = np.random.uniform(0, 255, size=(len(labels), 3))\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = f\"{labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "for img in imagePaths:\n",
    "    # Load the input image\n",
    "    # image = cv2.imread(\"2007_000033.jpg\")\n",
    "    image = cv2.imread(img)\n",
    "    # testLabels = ['aeroplane', 'aeroplane', 'aeroplane'] #TODO: cambiar luego con los reales, esto es de prueba para el accuracy\n",
    "    testLabels = labels\n",
    "    predicted_labels = [''] * len(testLabels)\n",
    "\n",
    "    # Get bounding boxes and detected classes\n",
    "    boxes, confidences, class_ids = get_objects(image)\n",
    "\n",
    "    # Apply non-maximum suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.4)\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "\n",
    "        # Filter the bounding boxes, confidences, and class IDs\n",
    "        boxes = [boxes[i] for i in indices]\n",
    "        confidences = [confidences[i] for i in indices]\n",
    "        class_ids = [class_ids[i] for i in indices]\n",
    "\n",
    "    else:\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "    '''boxes = [boxes[i] for i in indices.flatten()]\n",
    "    confidences = [confidences[i] for i in indices.flatten()]\n",
    "    class_ids = [class_ids[i] for i in indices.flatten()]'''\n",
    "\n",
    "    # Get the predicted labels\n",
    "    for i in range(len(class_ids)):\n",
    "        predicted_labels[i] = labels[class_ids[i]]\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    draw_boxes(image, boxes, confidences, class_ids)\n",
    "\n",
    "    # Show the resulting image with Matplotlib\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate accuracy and other metrics\n",
    "    accuracy = accuracy_score(testLabels, predicted_labels)\n",
    "    confusion_mat = confusion_matrix(testLabels, predicted_labels)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
