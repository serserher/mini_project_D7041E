{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base path to the input dataset and then use it to derive\n",
    "# the path to the input images and annotation CSV files\n",
    "BASE_PATH = \"VOC2012\"\n",
    "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"JPEGimages\"])\n",
    "IMAGES_PATH_REDUCED = os.path.sep.join([BASE_PATH, \"Images_Reduced\"])\n",
    "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"Annotations\"])\n",
    "ANNOTS_PATH_REDUCED = os.path.sep.join([BASE_PATH, \"Annotations_Reduced\"])\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
    "# define the path to the output model, label encoder, plots output\n",
    "# directory, and testing image paths\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.pth\"])\n",
    "LE_PATH = os.path.sep.join([BASE_OUTPUT, \"le.pickle\"])\n",
    "PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to preprocess the data in a way that we have the two datasets we need. First, the whole dataset with images with several objects and parts and other with the images with only one object and one part in it. This is done because the Pytorch architecture only works with one object per image, but YOLO handles multiple objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_single_bounding_box(object_element):\n",
    "    # Check if an object has only one bounding box (ignoring parts)\n",
    "    return len(object_element.findall('part')) == 0\n",
    "\n",
    "def filter_annotations(input_folder_annotations, output_folder_annotations, input_folder_images, output_folder_images, limit_number = 10000):\n",
    "    os.makedirs(output_folder_annotations, exist_ok=True)\n",
    "    os.makedirs(output_folder_images, exist_ok=True)\n",
    "    count = 0\n",
    "    for filename in os.listdir(input_folder_annotations):\n",
    "        if count >= limit_number:\n",
    "            continue\n",
    "        if filename.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(input_folder_annotations, filename)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Check if the annotation has only one object with a single bounding box\n",
    "            objects = root.findall('.//object')\n",
    "            if len(objects) == 1 and has_single_bounding_box(objects[0]):\n",
    "                output_path = os.path.join(output_folder_annotations, filename)\n",
    "                # Copy the XML file to the output folder\n",
    "                os.makedirs(output_folder_annotations, exist_ok=True)\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(ET.tostring(root))\n",
    "                   # Copy the corresponding JPEG image to the \"JPEGImages\" folder\n",
    "                image_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "                image_path_src = os.path.join(input_folder_images, image_filename)\n",
    "                image_path_dest = os.path.join(output_folder_images, image_filename)\n",
    "                shutil.copy(image_path_src, image_path_dest)\n",
    "                count += 1 \n",
    "input_folder_a = ANNOTS_PATH\n",
    "output_folder_a = ANNOTS_PATH_REDUCED\n",
    "\n",
    "input_folder_i = IMAGES_PATH\n",
    "output_folder_i = IMAGES_PATH_REDUCED\n",
    "\n",
    "filter_annotations(input_folder_a, output_folder_a, input_folder_i, output_folder_i, limit_number = 4000)\n",
    "\n",
    "IMAGES_PATH_REDUCED_TEST = os.path.sep.join([BASE_PATH, \"Images_Reduced_Test\"])\n",
    "ANNOTS_PATH_REDUCED_TEST = os.path.sep.join([BASE_PATH, \"Annotations_Reduced_Test\"])\n",
    "\n",
    "input_folder_a = ANNOTS_PATH\n",
    "output_folder_a = ANNOTS_PATH_REDUCED_TEST\n",
    "\n",
    "input_folder_i = IMAGES_PATH\n",
    "output_folder_i = IMAGES_PATH_REDUCED_TEST\n",
    "\n",
    "filter_annotations(input_folder_a, output_folder_a, input_folder_i, output_folder_i, limit_number = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do it for the reduced dataset to compare with the other architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "data = []\n",
    "labels = []\n",
    "bboxes = []\n",
    "imagePaths = []\n",
    "\n",
    "# loop over all XML files in the annotations directory\n",
    "for xmlPath in paths.list_files(ANNOTS_PATH_REDUCED, validExts=(\".xml\")):\n",
    "    # load the contents of the current XML annotations file\n",
    "    tree = ET.parse(xmlPath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # extract information from XML\n",
    "    filename = root.find(\"filename\").text\n",
    "    label = root.find(\".//name\").text\n",
    "    xmin = float(root.find(\".//xmin\").text)\n",
    "    ymin = float(root.find(\".//ymin\").text)\n",
    "    xmax = float(root.find(\".//xmax\").text)\n",
    "    ymax = float(root.find(\".//ymax\").text)\n",
    "\n",
    "    # derive the path to the input image, load the image (in OpenCV format), and grab its dimensions\n",
    "    imagePath = os.path.sep.join([IMAGES_PATH_REDUCED, filename])\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # scale the bounding box coordinates relative to the spatial dimensions of the input image\n",
    "    startX = xmin / w\n",
    "    startY = ymin / h\n",
    "    endX = xmax / w\n",
    "    endY = ymax / h\n",
    "\n",
    "    # load the image and preprocess it\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # update our list of data, class labels, bounding boxes, and image paths\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    bboxes.append((startX, startY, endX, endY))\n",
    "    imagePaths.append(imagePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster processing of data, the lists are converted into numpy arrays. Since the labels are in string format, we use scikit-learnâ€™s LabelEncoder to transform them into their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data, class labels, bounding boxes, and image paths to\n",
    "# NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "imagePaths = np.array(imagePaths)\n",
    "# perform label encoding on the labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "for image in imagePaths:\n",
    "    # Load image\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.resize(img, None, fx=0.4, fy=0.4)\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Showing informations on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(labels[class_ids[i]])\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0,255,0), 2)\n",
    "            cv2.putText(img, label, (x, y + 30), cv2.FONT_HERSHEY_PLAIN, 3, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    predicted_labels = [labels[i] for i in class_ids]\n",
    "    #TODO: falta sacar las reales y probar todo\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
